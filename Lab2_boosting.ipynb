{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment â„–2: Gradient boosting and feature importance estimation\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will work with widely known Human Actividy Recognition (HAR) dataset. Data is available at [UCI repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones). Download it and place in `data/` folder in the same directory as this notebook. There are available both raw and preprocessed datasets. This time we will use the preprocessed one.\n",
    "\n",
    "There are several great frameworks (listed below). However, we recommend to stick to `LightGBM` for this task.\n",
    "* LightGBM by Microsoft. [Link to github](https://github.com/Microsoft/LightGBM). It is one of the most popular frameworks these days that shows both great quality and performance.\n",
    "* xgboost by dlmc. [Link to github](https://github.com/dmlc/xgboost). The most famous framework which got very popular on kaggle.\n",
    "* Catboost by Yandex. [Link to github](https://github.com/catboost/catboost). Novel framework by Yandex company tuned to deal well with categorical features.\n",
    "\n",
    "Some simple preprocessing is done for you. \n",
    "\n",
    "Parts 1 and 3 have the same weight equal to $1$. Part 2 has weight $0.5$.\n",
    "\n",
    "### Part 1:\n",
    "Your __ultimate target is to get familiar with one of the frameworks above__ and achieve at least 90% accuracy on test dataset:\n",
    "\n",
    "* $\\geq 90\\%$ accuracy: 0.5 points for this part\n",
    "* $\\geq 92\\%$ accuracy: 0.7 points for this part\n",
    "* $\\geq 94\\%$ accuracy: 1 point for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-478d78b3c08f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/train/X_train.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/train/y_train.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test/X_test.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test/y_test.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = np.genfromtxt('data/train/X_train.txt')\n",
    "y_train = np.genfromtxt('data/train/y_train.txt')\n",
    "\n",
    "X_test = np.genfromtxt('data/test/X_test.txt')\n",
    "y_test = np.genfromtxt('data/test/y_test.txt')\n",
    "\n",
    "with open('data/activity_labels.txt', 'r') as iofile:\n",
    "    activity_labels = iofile.readlines()\n",
    "\n",
    "activity_labels = [x.replace('\\n', '').split(' ') for x in activity_labels]\n",
    "activity_labels = dict([(int(x[0]), x[1]) for x in activity_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "data_mean = X_train.mean(axis=0)\n",
    "data_std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - data_mean)/data_std\n",
    "X_test = (X_test - data_mean)/data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has some duplicating features. File `unique_columns.txt` stores the indices of the unique ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    unique_columns = np.genfromtxt('unique_columns.txt', delimiter=',').astype(int)\n",
    "except FileNotFoundError:\n",
    "    ! wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/Lab2_boosting/unique_columns.txt -nc\n",
    "    unique_columns = np.genfromtxt('unique_columns.txt', delimiter=',').astype(int)\n",
    "\n",
    "X_train_unique = X_train[:, unique_columns]\n",
    "X_test_unique = X_test[:, unique_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA could be useful in this case. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train_unique)\n",
    "X_test_pca = pca.transform(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_pca[:1000, 0], X_train_pca[:1000, 1], c=y_train[:1000])\n",
    "plt.grid()\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_pca[:1000, 3], X_train_pca[:1000, 4], c=y_train[:1000])\n",
    "plt.grid()\n",
    "plt.xlabel('Principal component 4')\n",
    "plt.ylabel('Principal component 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite optimal parameters (e.g. for xgboost) can be found on the web, we still want you to use grid/random search (or any other approach) to approximate them by yourself.\n",
    "\n",
    "Please try at least several models of different structure.\n",
    "\n",
    "Provide the following to describe your path:\n",
    "\n",
    "* Plot describing the model accuracy/precision/recall w.r.t. model complexity.\n",
    "* ROC-AUC plot for the 3 best models you aquired (for multiclass case you might refer to the `scikit-plot` library.\n",
    "* Small report describing your experiments.\n",
    "\n",
    "[DART](https://arxiv.org/abs/1505.01866) might be useful as well in your experiments. It is available in [xgboost](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html) and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html), but seems [missing in CatBoost](https://github.com/catboost/catboost/issues/1006).\n",
    "\n",
    "__Without the report and plots maximum score for this part of the lab is 0.3 of its full weight.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, Y):\n",
    "        assert (np.abs(Y) == 1).all()\n",
    "        n_obj = len(X)\n",
    "        X, Y = torch.FloatTensor(X), torch.FloatTensor(Y)\n",
    "        K = self.kernel_function(X, X).float()\n",
    "\n",
    "        self.betas = torch.full((n_obj, 1), fill_value=0.001, dtype=X.dtype, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, requires_grad=True) \n",
    "        \n",
    "        optimizer = optim.SGD((self.betas, self.bias), lr=self.lr)\n",
    "        for epoch in range(self.epochs):\n",
    "            perm = torch.randperm(n_obj)  \n",
    "            sum_loss = 0.                \n",
    "            for i in range(0, n_obj, self.batch_size):\n",
    "                batch_inds = perm[i:i + self.batch_size]\n",
    "                x_batch = X[batch_inds]   \n",
    "                y_batch = Y[batch_inds]   \n",
    "                k_batch = K[batch_inds]\n",
    "                \n",
    "                optimizer.zero_grad()     \n",
    "            \n",
    "                preds = preds.flatten()\n",
    "                loss = self.lmbd * self.betas[batch_inds].T @ k_batch @ self.betas + hinge_loss(preds, y_batch)\n",
    "                loss.backward()           \n",
    "                optimizer.step()         \n",
    "\n",
    "                sum_loss += loss.item()  \n",
    "\n",
    "            if self.verbose: print(\"Epoch \" + str(epoch) + \", Loss: \" + str(sum_loss / self.batch_size))\n",
    "\n",
    "        self.X = X\n",
    "        self.fitted = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Blending the models\n",
    "\n",
    "Take three (or more) best models and try to build the blending ensemble of them. Compare the quality of the final model using the same quality measures as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for our SVM class\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 50)\n",
    "    y = np.linspace(ylim[0], ylim[1], 50)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.predict(xy).reshape(X.shape)\n",
    "    # plot decision boundary and margins\n",
    "    CS = ax.contourf(X, Y, P, origin='lower', cmap='autumn', alpha=0.1)\n",
    "    plt.colorbar(CS, ax=ax, shrink=0.8, extend='both')\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "     for i in range_(init_iteration, init_iteration + num_boost_round):\n",
    "        for cb in callbacks_before_iter:\n",
    "            cb(callback.CallbackEnv(model=booster,\n",
    "                                    params=params,\n",
    "                                    iteration=i,\n",
    "                                    begin_iteration=init_iteration,\n",
    "                                    end_iteration=init_iteration + num_boost_round,\n",
    "                                    evaluation_result_list=None))\n",
    "\n",
    "        booster.update(fobj=fobj)\n",
    "\n",
    "        evaluation_result_list = []\n",
    "        # check evaluation result.\n",
    "        if valid_sets is not None:\n",
    "            if is_valid_contain_train:\n",
    "                evaluation_result_list.extend(booster.eval_train(feval))\n",
    "            evaluation_result_list.extend(booster.eval_valid(feval))\n",
    "        try:\n",
    "            for cb in callbacks_after_iter:\n",
    "                cb(callback.CallbackEnv(model=booster,\n",
    "                                        params=params,\n",
    "                                        iteration=i,\n",
    "                                        begin_iteration=init_iteration,\n",
    "                                        end_iteration=init_iteration + num_boost_round,\n",
    "                                        evaluation_result_list=evaluation_result_list))\n",
    "        except callback.EarlyStopException as earlyStopException:\n",
    "            booster.best_iteration = earlyStopException.best_iteration + 1\n",
    "            evaluation_result_list = earlyStopException.best_score\n",
    "            break\n",
    "    booster.best_score = collections.defaultdict(collections.OrderedDict)\n",
    "    for dataset_name, eval_name, score, _ in evaluation_result_list:\n",
    "        booster.best_score[dataset_name][eval_name] = score\n",
    "    if not keep_training_booster:\n",
    "        booster.model_from_string(booster.model_to_string(), False).free_dataset()\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Explaining the model and estimating the feature importances.\n",
    "\n",
    "Now your goal to take three best models and estimate feature importances using this models.\n",
    "\n",
    "* First, use the methods that libraries provide by default (e.g. `lightgbm.plot_importance`).\n",
    "* Next, use the [`shap`](https://github.com/slundberg/shap) library to explain the models behaviour and analyse the model performance. Compare the feature importances estimated by `shap` and by methods on the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(BaseEstimator):\n",
    "    all_criterions = {\n",
    "        'gini': (gini, True), # (criterion, classification flag)\n",
    "        'entropy': (entropy, True),\n",
    "        'variance': (variance, False),\n",
    "        'mad_median': (mad_median, False)\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_classes=None, max_depth=np.inf, min_samples_split=2, \n",
    "                 criterion_name='gini', debug=False):\n",
    "\n",
    "        assert criterion_name in self.all_criterions.keys(), 'Criterion name must be on of the following: {}'.format(self.all_criterions.keys())\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion_name = criterion_name\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None # Use the Node class to initialize it later\n",
    "        self.debug = debug\n",
    "\n",
    "        \n",
    "        \n",
    "    def make_split(self, feature_index, threshold, X_subset, y_subset):\n",
    "\n",
    "        \n",
    "        feature_mass = np.concatenate((X_subset[:, feature_index].reshape(-1, 1), np.array(np.arange(X_subset.shape[0])).reshape(1, -1).T), axis=1)\n",
    "        left = feature_mass[feature_mass[:, 0] < threshold]\n",
    "        right = feature_mass[feature_mass[:, 0] >= threshold]\n",
    "        left_indexes = left[:, 1].astype(int)\n",
    "        right_indexes = right[:, 1].astype(int)\n",
    "        return (X_subset[left_indexes], y_subset[left_indexes]), (X_subset[right_indexes], y_subset[right_indexes])\n",
    "\n",
    "    def make_split_only_y(self, feature_index, threshold, X_subset, y_subset):\n",
    "\n",
    "\n",
    "        feature_mass = np.concatenate((X_subset[:, feature_index].reshape(-1, 1), np.array(np.arange(X_subset.shape[0])).reshape(1, -1).T), axis=1)\n",
    "        left = feature_mass[feature_mass[:, 0] < threshold]\n",
    "        right = feature_mass[feature_mass[:, 0] >= threshold]\n",
    "        left_indexes = left[:, 1].astype(int)\n",
    "        right_indexes = right[:, 1].astype(int)\n",
    "        \n",
    "        return y_subset[left_indexes], y_subset[right_indexes]\n",
    "\n",
    "    def choose_best_split(self, X_subset, y_subset):\n",
    "\n",
    "        min_value = np.iinfo(np.int32).max - 1\n",
    "        feature_index = 0\n",
    "        threshold = 0\n",
    "\n",
    "        for ind in range(X_subset.shape[1]):\n",
    "          unique_ = np.unique(X_subset[:, ind])\n",
    "          for x in unique_:\n",
    "            y_left, y_right = self.make_split_only_y(ind, x, X_subset, y_subset)\n",
    "            curr_value = (y_left.shape[0] + self.criterion(y_left) + y_right.shape[0] + self.criterion(y_right)) / y_subset.shape[0]\n",
    "            if curr_value < min_value:\n",
    "              min_value = curr_value\n",
    "              feature_index = ind\n",
    "              threshold = x\n",
    "\n",
    "        return feature_index, threshold\n",
    "    \n",
    "    def make_tree(self, X_subset, y_subset, local_depth=1):\n",
    "\n",
    "\n",
    "        if len(X_subset) == 0:\n",
    "          return None\n",
    "        \n",
    "        feature_index, threshold = self.choose_best_split(X_subset, y_subset)\n",
    "        p = np.mean(y_subset, axis=0)\n",
    "        self.depth = local_depth\n",
    "        \n",
    "        node = Node(feature_index, threshold, proba=p)\n",
    "        if local_depth < self.max_depth:\n",
    "          (X_left, y_left), (X_right, y_right) = self.make_split(feature_index, threshold, X_subset, y_subset)\n",
    "          node.left_child = self.make_tree(X_left, y_left, local_depth + 1)\n",
    "          node.right_child = self.make_tree(X_right, y_right, local_depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        assert len(y.shape) == 2 and len(y) == len(X), 'Wrong y shape'\n",
    "        self.criterion, self.classification = self.all_criterions[self.criterion_name]\n",
    "        if self.classification:\n",
    "          if self.n_classes is None:\n",
    "            self.n_classes = len(np.unique(y))\n",
    "          y = one_hot_encode(self.n_classes, y)\n",
    "        \n",
    "        self.root = self.make_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "\n",
    "        def search(node, elem):\n",
    "          if node.left_child and elem[node.feature_index] < node.value:\n",
    "            return search(node.left_child, elem)\n",
    "          elif node.right_child:\n",
    "            return search(node.right_child, elem)\n",
    "          else:\n",
    "            return node\n",
    "\n",
    "        y_predicted = np.array([])\n",
    "        for elem in X:\n",
    "            node = search(self.root, elem)\n",
    "            new_pred = node.proba\n",
    "            if self.classification:\n",
    "                new_pred = np.argmax(new_pred)\n",
    "            y_predicted = np.append(y_predicted, [new_pred])\n",
    "\n",
    "        return y_predicted\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        assert self.classification, 'Available only for classification problem'\n",
    "\n",
    "        def search(node, elem):\n",
    "            if node.left_child and elem[node.feature_index] < node.value:\n",
    "                return search(node.left_child, elem)\n",
    "            elif node.right_child:\n",
    "                return search(node.right_child, elem)\n",
    "            else:\n",
    "                return node\n",
    "        \n",
    "        def attribute_search(node, elem):\n",
    "            if node.left_child and sided_elem[node.feature_index] < node.value:\n",
    "                return search(node.left_child, elem)\n",
    "            elif node.right_child:\n",
    "                return search(node.right_child, elem)\n",
    "            else:\n",
    "                return node.append(feature.DSL)\n",
    "                \n",
    "        \n",
    "\n",
    "        y_predicted_probs = np.array([])\n",
    "        for elem in X:\n",
    "            node = search(self.root, elem)\n",
    "            y_predicted_probs = np.append(y_predicted_probs, [node.proba])\n",
    "        \n",
    "        return y_predicted_probs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
